# 04. Ph√¢n T√≠ch Xu H∆∞·ªõng & Keyword

T√†i li·ªáu n√†y h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng module ph√¢n t√≠ch xu h∆∞·ªõng ƒë·ªÉ t√¨m ra c√°c ch·ªß ƒë·ªÅ, keyword ph·ªï bi·∫øn t·ª´ tin t·ª©c.

---

## 1. TrendingAnalyzer - Kh√°i Ni·ªám

### TrendingAnalyzer L√† G√¨?

`TrendingAnalyzer` l√† c√¥ng c·ª• ph√¢n t√≠ch d·ªØ li·ªáu tin t·ª©c ƒë·ªÉ t√¨m ra:

1. **T·ª´ kh√≥a ph·ªï bi·∫øn** (keywords) - T·ª´/c·ª•m t·ª´ xu·∫•t hi·ªán nhi·ªÅu
2. **Ch·ªß ƒë·ªÅ xu h∆∞·ªõng** (trending topics) - C√°c ch·ªß ƒë·ªÅ ƒë∆∞·ª£c b√†n lu·∫≠n
3. **ƒê·ªô ph·ªï bi·∫øn c·ªßa t·ª´** - T·∫ßn su·∫•t t·ª´ xu·∫•t hi·ªán

### Ho·∫°t ƒê·ªông

```
Danh s√°ch tin t·ª©c (DataFrame)
    ‚Üì
L·∫•y n·ªôi dung t·ª´ c√°c b√†i
    ‚Üì
Chu·∫©n h√≥a text (th∆∞·ªùng d√πng, x√≥a k√Ω t·ª± l·∫°)
    ‚Üì
T√°ch t·ª´ (tokenization) & x√≥a t·ª´ kh√¥ng c·∫ßn (stopwords)
    ‚Üì
ƒê·∫øm t·∫ßn su·∫•t t·ª´
    ‚Üì
X·∫øp h·∫°ng & tr·∫£ v·ªÅ k·∫øt qu·∫£
```

---

## 2. Kh·ªüi T·∫°o & C∆° B·∫£n

### Import

```python
from vnstock_news import TrendingAnalyzer
import pandas as pd
```

### Kh·ªüi T·∫°o

```python
# C√°ch 1: M·∫∑c ƒë·ªãnh (Vietnamese)
analyzer = TrendingAnalyzer()

# C√°ch 2: Custom language & stopwords
analyzer = TrendingAnalyzer(
    language='vietnamese',
    min_frequency=2,          # T·ª´ ph·∫£i xu·∫•t hi·ªán √≠t nh·∫•t 2 l·∫ßn
    max_keywords=20,          # Tr·∫£ v·ªÅ t·ªëi ƒëa 20 t·ª´
    min_word_length=2         # T·ª´ ph·∫£i c√≥ √≠t nh·∫•t 2 k√Ω t·ª±
)
```

**Parameters:**
- `language` (str): Ng√¥n ng·ªØ ('vietnamese', 'english', ...)
- `min_frequency` (int): T·ª´ ph·∫£i xu·∫•t hi·ªán √≠t nh·∫•t N l·∫ßn ƒë·ªÉ ƒë∆∞·ª£c coi l√† trending
- `max_keywords` (int): Tr·∫£ v·ªÅ t·ªëi ƒëa bao nhi√™u keyword
- `min_word_length` (int): T·ª´ ph·∫£i d√†i t·ªëi thi·ªÉu bao nhi√™u k√Ω t·ª±

---

## 3. Ph∆∞∆°ng Th·ª©c Ch√≠nh

### `extract_keywords(texts, top_n=20)`

Tr√≠ch xu·∫•t keywords t·ª´ danh s√°ch text.

```python
from vnstock_news import Crawler, TrendingAnalyzer

# B∆∞·ªõc 1: L·∫•y tin t·ª©c
crawler = Crawler(site_name="cafef")
articles = crawler.get_articles_from_feed(limit_per_feed=30)

# B∆∞·ªõc 2: Kh·ªüi t·∫°o analyzer
analyzer = TrendingAnalyzer()

# B∆∞·ªõc 3: Tr√≠ch xu·∫•t keywords
# D√πng 'content' (n·ªôi dung ƒë·∫ßy ƒë·ªß) ho·∫∑c 'title' (ti√™u ƒë·ªÅ)
keywords = analyzer.extract_keywords(
    texts=articles['title'].tolist(),
    top_n=20
)

print(keywords)
```

**Output:**
```
{'ch·ª©ng kho√°n': 45, 'nh√† ƒë·∫ßu t∆∞': 32, 'th·ªã tr∆∞·ªùng': 28, 'tƒÉng': 25, ...}
```

**Parameters:**
- `texts` (list): Danh s√°ch text c·∫ßn ph√¢n t√≠ch
- `top_n` (int): Tr·∫£ v·ªÅ top N keyword (default: 20)

**Returns:** Dict with format `{keyword: frequency}`

---

### `extract_topics(articles_df, content_column='content', top_n=20)`

Tr√≠ch xu·∫•t topics t·ª´ DataFrame tin t·ª©c.

```python
from vnstock_news import Crawler, TrendingAnalyzer

crawler = Crawler(site_name="tuoitre")
articles = crawler.get_articles_from_feed(limit_per_feed=50)

analyzer = TrendingAnalyzer()

topics = analyzer.extract_topics(
    articles_df=articles,
    content_column='content',  # C·ªôt ch·ª©a n·ªôi dung
    top_n=15
)

print(f"üî• Top topics:")
for i, (topic, count) in enumerate(topics.items(), 1):
    print(f"{i}. {topic}: {count}")
```

**Output:**
```
üî• Top topics:
1. ch·ª©ng kho√°n: 45
2. nh√† ƒë·∫ßu t∆∞: 32
3. th·ªã tr∆∞·ªùng: 28
4. tƒÉng: 25
5. c√¥ng ty: 22
```

---

### `get_trending(articles_df, time_window='24h', top_n=20)`

L·∫•y trending **trong kho·∫£ng th·ªùi gian c·ª• th·ªÉ**.

```python
from datetime import datetime
from vnstock_news import Crawler, TrendingAnalyzer

crawler = Crawler(site_name="cafef")
articles = crawler.get_articles_from_sitemap(limit=500)

analyzer = TrendingAnalyzer()

# L·∫•y trending t·ª´ 7 ng√†y g·∫ßn ƒë√¢y
trending = analyzer.get_trending(
    articles_df=articles,
    time_window='7d',  # '24h', '7d', '30d', '90d'
    top_n=25
)

print("üìä Trending last 7 days:")
for keyword, stats in trending.items():
    print(f"{keyword}: {stats['count']} occurrences")
```

**Output:**
```
üìä Trending last 7 days:
ch·ª©ng kho√°n: {'count': 45, 'first_mention': 2025-01-15, 'last_mention': 2025-01-22}
nh√† ƒë·∫ßu t∆∞: {'count': 32, ...}
```

**Parameters:**
- `articles_df` (DataFrame): DataFrame ch·ª©a tin t·ª©c (ph·∫£i c√≥ c·ªôt 'publish_time')
- `time_window` (str): Kho·∫£ng th·ªùi gian ('24h', '7d', '30d', '90d')
- `top_n` (int): Top N keywords

---

### `analyze_sentiment(texts)`

Ph√¢n t√≠ch c·∫£m x√∫c (Positive/Negative) trong text.

```python
analyzer = TrendingAnalyzer()

texts = [
    "Ch·ª©ng kho√°n tƒÉng m·∫°nh h√¥m nay",  # T√≠ch c·ª±c
    "Th·ªã tr∆∞·ªùng suy tho√°i, nh√† ƒë·∫ßu t∆∞ lo l·∫Øng",  # Ti√™u c·ª±c
]

sentiments = analyzer.analyze_sentiment(texts)

print(sentiments)
# [{'text': '...', 'sentiment': 'positive', 'score': 0.85},
#  {'text': '...', 'sentiment': 'negative', 'score': -0.72}]
```

---

## 4. V√≠ D·ª• Th·ª±c T·∫ø

### V√≠ D·ª• 1: T√¨m Keyword Ph·ªï Bi·∫øn H√¥m Nay

```python
from vnstock_news import Crawler, TrendingAnalyzer
import pandas as pd

# B∆∞·ªõc 1: L·∫•y tin m·ªõi nh·∫•t
crawler = Crawler(site_name="cafef")
articles = crawler.get_articles_from_feed(limit_per_feed=50)

print(f"üì∞ L·∫•y {len(articles)} b√†i vi·∫øt")

# B∆∞·ªõc 2: Ph√¢n t√≠ch
analyzer = TrendingAnalyzer(min_frequency=2)
keywords = analyzer.extract_keywords(
    texts=articles['title'].tolist(),
    top_n=20
)

# B∆∞·ªõc 3: Hi·ªÉn th·ªã
print("\nüî• C√°c t·ª´ kh√≥a h√¥m nay:\n")
for i, (keyword, count) in enumerate(keywords.items(), 1):
    print(f"{i:2d}. {keyword:20s} - {count:3d} l·∫ßn")

# B∆∞·ªõc 4: L∆∞u
result_df = pd.DataFrame(
    [{'keyword': k, 'frequency': v} for k, v in keywords.items()]
)
result_df.to_csv("trending_keywords.csv", index=False)
```

**Output:**
```
üì∞ L·∫•y 47 b√†i vi·∫øt

üî• C√°c t·ª´ kh√≥a h√¥m nay:

 1. ch·ª©ng kho√°n       -  45 l·∫ßn
 2. nh√† ƒë·∫ßu t∆∞        -  32 l·∫ßn
 3. th·ªã tr∆∞·ªùng        -  28 l·∫ßn
 4. tƒÉng              -  25 l·∫ßn
 5. gi√° c·ªï phi·∫øu      -  20 l·∫ßn
 ...
```

---

### V√≠ D·ª• 2: So S√°nh Trending Gi·ªØa 2 B√°o

```python
from vnstock_news import Crawler, TrendingAnalyzer
import pandas as pd

analyzer = TrendingAnalyzer()

# L·∫•y t·ª´ CafeF
cafef_crawler = Crawler(site_name="cafef")
cafef_articles = cafef_crawler.get_articles_from_feed(limit_per_feed=30)
cafef_keywords = analyzer.extract_keywords(cafef_articles['title'].tolist(), top_n=10)

# L·∫•y t·ª´ VietStock
vietstock_crawler = Crawler(site_name="vietstock")
vietstock_articles = vietstock_crawler.get_articles_from_feed(limit_per_feed=30)
vietstock_keywords = analyzer.extract_keywords(vietstock_articles['title'].tolist(), top_n=10)

# So s√°nh
print("CafeF Top 10:")
for i, (k, v) in enumerate(list(cafef_keywords.items())[:10], 1):
    print(f"  {i}. {k}: {v}")

print("\nVietStock Top 10:")
for i, (k, v) in enumerate(list(vietstock_keywords.items())[:10], 1):
    print(f"  {i}. {k}: {v}")

# Keywords ch·ªâ c√≥ ·ªü CafeF
cafef_only = set(cafef_keywords.keys()) - set(vietstock_keywords.keys())
print(f"\nCh·ªâ CafeF b√†n lu·∫≠n: {cafef_only}")
```

---

### V√≠ D·ª• 3: Trending Theo Th·ªùi Gian (Time Series)

```python
from vnstock_news import Crawler, TrendingAnalyzer
from datetime import datetime, timedelta
import pandas as pd

crawler = Crawler(site_name="cafef")

# L·∫•y tin t·ª´ 30 ng√†y g·∫ßn ƒë√¢y
articles = crawler.get_articles_from_sitemap(limit=1000)

analyzer = TrendingAnalyzer()

# Ph√¢n t√≠ch t·ª´ng tu·∫ßn
trending_by_week = {}

for i in range(4):  # 4 tu·∫ßn
    start_date = datetime.now() - timedelta(days=7*(i+1))
    end_date = datetime.now() - timedelta(days=7*i)
    
    # L·ªçc b√†i trong kho·∫£ng th·ªùi gian
    week_articles = articles[
        (articles['publish_time'] >= start_date) & 
        (articles['publish_time'] < end_date)
    ]
    
    if len(week_articles) > 0:
        keywords = analyzer.extract_keywords(
            week_articles['title'].tolist(),
            top_n=5
        )
        
        week_name = f"Week {4-i} ({start_date.strftime('%d/%m')} - {end_date.strftime('%d/%m')})"
        trending_by_week[week_name] = keywords

# Hi·ªÉn th·ªã
for week, keywords in trending_by_week.items():
    print(f"\n{week}:")
    for k, v in keywords.items():
        print(f"  - {k}: {v}")
```

**Output:**
```
Week 1 (08/01 - 15/01):
  - ch·ª©ng kho√°n: 45
  - nh√† ƒë·∫ßu t∆∞: 32

Week 2 (01/01 - 08/01):
  - l·∫°m ph√°t: 28
  - l√£i su·∫•t: 25
```

---

### V√≠ D·ª• 4: Real-time Monitoring V·ªõi vnstock_news Main

```python
# main.py c√≥ script news_monitor s·∫µn
from vnstock_news.main import news_monitor

# Ho·∫∑c s·ª≠ d·ª•ng tr·ª±c ti·∫øp:
from vnstock_news import Crawler, TrendingAnalyzer
import asyncio
from datetime import datetime

async def monitor_news():
    """Monitor tin t·ª©c theo th·ªùi gian th·ª±c"""
    
    sites = ["cafef", "tuoitre", "vietstock"]
    
    crawler = Crawler()
    analyzer = TrendingAnalyzer()
    
    article_history = []
    
    while True:
        print(f"\n{'='*60}")
        print(f"üì∞ Monitoring at {datetime.now().strftime('%H:%M:%S')}")
        print(f"{'='*60}")
        
        for site_name in sites:
            try:
                c = Crawler(site_name=site_name)
                articles = c.get_articles_from_feed(limit_per_feed=20)
                
                keywords = analyzer.extract_keywords(
                    articles['title'].tolist(),
                    top_n=5
                )
                
                print(f"\nüìä {site_name.upper()}:")
                for keyword, count in keywords.items():
                    print(f"  {keyword}: {count}")
                
                article_history.extend(articles.to_dict('records'))
                
            except Exception as e:
                print(f"‚ùå Error fetching {site_name}: {e}")
        
        # T·ªïng th·ªÉ trending
        if len(article_history) > 0:
            print(f"\nüî• OVERALL TRENDING (t·ª´ {len(article_history)} b√†i):")
            all_titles = [a['title'] for a in article_history]
            overall_keywords = analyzer.extract_keywords(all_titles, top_n=10)
            
            for i, (keyword, count) in enumerate(overall_keywords.items(), 1):
                print(f"  {i}. {keyword}: {count}")
        
        # Ch·ªù 60 ph√∫t r·ªìi l·∫∑p l·∫°i
        print("\n‚è≥ Ch·ªù 1 gi·ªù...")
        await asyncio.sleep(3600)

# Ch·∫°y
asyncio.run(monitor_news())
```

---

## 5. T·ªëi ∆Øu H√≥a K·∫øt Qu·∫£

### Lo·∫°i B·ªè T·ª´ Kh√¥ng C·∫ßn

M·ªôt s·ªë t·ª´ kh√¥ng mang √Ω nghƒ©a (stopwords) n√™n lo·∫°i b·ªè:

```python
analyzer = TrendingAnalyzer(
    min_frequency=3,      # Lo·∫°i b·ªè t·ª´ xu·∫•t hi·ªán < 3 l·∫ßn
    min_word_length=3,    # Lo·∫°i b·ªè t·ª´ < 3 k√Ω t·ª±
    language='vietnamese'
)

# D√πng custom stopwords
from vnstock_news import TrendingAnalyzer

custom_stopwords = ['l√†', 'v√†', 'c·ªßa', 'b·ªã', 'ƒë∆∞·ª£c', 'c√≥', 'c√°i', 'n√†y', 'ƒë√≥']

analyzer = TrendingAnalyzer(
    custom_stopwords=custom_stopwords
)
```

### Chu·∫©n H√≥a Text

```python
def preprocess_text(text):
    """Chu·∫©n h√≥a text"""
    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    text = text.lower()
    
    # X√≥a d·∫•u ngo·∫∑c, k√Ω t·ª± ƒë·∫∑c bi·ªát
    import re
    text = re.sub(r'[^a-z√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë\s]', '', text)
    
    # X√≥a kho·∫£ng tr·∫Øng th·ª´a
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# S·ª≠ d·ª•ng
articles['title_clean'] = articles['title'].apply(preprocess_text)
keywords = analyzer.extract_keywords(articles['title_clean'].tolist())
```

---

## 6. Xu·∫•t K·∫øt Qu·∫£

### Export CSV

```python
from vnstock_news import TrendingAnalyzer
import pandas as pd

keywords = {
    'ch·ª©ng kho√°n': 45,
    'nh√† ƒë·∫ßu t∆∞': 32,
    'th·ªã tr∆∞·ªùng': 28
}

# Chuy·ªÉn sang DataFrame
df = pd.DataFrame([
    {'keyword': k, 'frequency': v, 'percentage': v/sum(keywords.values())*100}
    for k, v in keywords.items()
])

df = df.sort_values('frequency', ascending=False)

# L∆∞u
df.to_csv('keywords.csv', index=False, encoding='utf-8-sig')

print(df)
```

**Output CSV:**
```
keyword,frequency,percentage
ch·ª©ng kho√°n,45,42.86
nh√† ƒë·∫ßu t∆∞,32,30.48
th·ªã tr∆∞·ªùng,28,26.67
```

---

### Visualize v·ªõi Matplotlib

```python
import matplotlib.pyplot as plt
from vnstock_news import TrendingAnalyzer

keywords = analyzer.extract_keywords(articles['title'].tolist(), top_n=10)

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(12, 6))
plt.barh(list(keywords.keys()), list(keywords.values()), color='steelblue')
plt.xlabel('Frequency')
plt.title('Trending Keywords')
plt.tight_layout()
plt.savefig('keywords.png', dpi=100)
plt.show()
```

---

## 7. Best Practices

| ‚úÖ N√™n L√†m | ‚ùå Kh√¥ng N√™n |
|-----------|-------------|
| Ph√¢n t√≠ch t·ª´ n·ªôi dung ƒë·∫ßy ƒë·ªß (content) | Ch·ªâ ph√¢n t√≠ch ti√™u ƒë·ªÅ (c√≥ th·ªÉ thi·∫øu ng·ªØ c·∫£nh) |
| Lo·∫°i b·ªè stopwords | Gi·ªØ t·∫•t c·∫£ c√°c t·ª´ |
| Chu·∫©n h√≥a text tr∆∞·ªõc ph√¢n t√≠ch | D√πng text th√¥ |
| L·ªçc theo th·ªùi gian tr∆∞·ªõc ph√¢n t√≠ch | Ph√¢n t√≠ch t·∫•t c·∫£ c√πng l√∫c |
| ƒê·∫∑t min_frequency ph√π h·ª£p | ƒê·∫∑t qu√° cao ho·∫∑c qu√° th·∫•p |
| K·∫øt h·ª£p nhi·ªÅu ngu·ªìn tin | Ch·ªâ d√πng 1 b√°o |

---

## 8. Troubleshooting

| V·∫•n ƒë·ªÅ | Nguy√™n Nh√¢n | Gi·∫£i Ph√°p |
|-------|-----------|---------|
| Keyword kh√¥ng h·ª£p l√Ω | Stopwords kh√¥ng ƒë·ªß | Th√™m custom stopwords |
| Qu√° nhi·ªÅu keyword t·∫ßm th∆∞·ªùng | min_frequency qu√° th·∫•p | TƒÉng min_frequency |
| Thi·∫øu keyword quan tr·ªçng | min_frequency qu√° cao | Gi·∫£m min_frequency |
| L·ªói ti·∫øng Vi·ªát | Encoding sai | D√πng utf-8, g·ªçi preprocess_text() |
| Ch·∫≠m | Ph√¢n t√≠ch qu√° nhi·ªÅu text | Gi·∫£m s·ªë l∆∞·ª£ng text, d√πng parallel processing |

---

## T·ªïng K·∫øt

**TrendingAnalyzer** gi√∫p b·∫°n:
- ‚úÖ T√¨m keyword ph·ªï bi·∫øn t·ª´ tin t·ª©c
- ‚úÖ Ph√°t hi·ªán xu h∆∞·ªõng theo th·ªùi gian
- ‚úÖ So s√°nh trending gi·ªØa c√°c b√°o
- ‚úÖ Monitor tin t·ª©c real-time
- ‚úÖ Ph√¢n t√≠ch c·∫£m x√∫c

**Workflow c∆° b·∫£n:**
```python
1. L·∫•y tin ‚Üí Crawler.get_articles_from_feed()
2. Ph√¢n t√≠ch ‚Üí TrendingAnalyzer.extract_keywords()
3. L∆∞u k·∫øt qu·∫£ ‚Üí DataFrame.to_csv()
4. Visualize ‚Üí Matplotlib/Pandas plot
```
