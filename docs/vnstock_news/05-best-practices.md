# 05. Best Practices & Tips

T√†i li·ªáu n√†y t·ªïng h·ª£p c√°c k·ªπ thu·∫≠t t·ªët nh·∫•t ƒë·ªÉ s·ª≠ d·ª•ng vnstock_news hi·ªáu qu·∫£ v√† an to√†n.

---

## 1. Chi·∫øn L∆∞·ª£c Thu Th·∫≠p D·ªØ Li·ªáu

### Ch·ªçn Ph∆∞∆°ng Th·ª©c Ph√π H·ª£p

| M·ª•c Ti√™u | Ph∆∞∆°ng Th·ª©c | ∆Øu ƒêi·ªÉm | Nh∆∞·ª£c ƒêi·ªÉm |
|----------|-------------|---------|-----------|
| C·∫≠p nh·∫≠t h√†ng ng√†y | RSS | Nhanh, c·∫≠p nh·∫≠t t·ª± ƒë·ªông | Ch·ªâ b√†i m·ªõi |
| X√¢y d·ª±ng database | Sitemap batch | L·∫•y l·ªãch s·ª≠, ƒë·∫ßy ƒë·ªß | Ch·∫≠m, ph·ª©c t·∫°p |
| Real-time monitoring | Async batch | Nhanh, concurrent | Ph·ª©c t·∫°p |
| S·∫£n xu·∫•t h√†ng ng√†y | Scheduler + RSS | T·ª± ƒë·ªông, tin t∆∞∆°i | C·∫ßn thi·∫øt l·∫≠p |

---

### V√≠ D·ª• 1: Daily Update (RSS + Scheduler)

```python
# daily_update.py
from vnstock_news import Crawler
from apscheduler.schedulers.background import BackgroundScheduler
import pandas as pd
from datetime import datetime
import os

scheduler = BackgroundScheduler()

def update_news_daily():
    """C·∫≠p nh·∫≠t tin h√†ng ng√†y"""
    
    print(f"‚è∞ Update at {datetime.now()}")
    
    sites = ["cafef", "tuoitre", "vietstock"]
    all_articles = []
    
    for site_name in sites:
        try:
            crawler = Crawler(site_name=site_name)
            articles = crawler.get_articles_from_feed(limit_per_feed=20)
            articles['source'] = site_name
            all_articles.append(articles)
            print(f"‚úÖ {site_name}: {len(articles)} articles")
        except Exception as e:
            print(f"‚ùå {site_name}: {e}")
    
    if all_articles:
        result = pd.concat(all_articles, ignore_index=True)
        
        # L∆∞u v√†o file theo ng√†y
        date_str = datetime.now().strftime("%Y-%m-%d")
        filename = f"news_updates/{date_str}.csv"
        
        os.makedirs("news_updates", exist_ok=True)
        result.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"üíæ Saved to {filename}")

# L√™n l·ªãch ch·∫°y m·ªói ng√†y l√∫c 8:00 AM
scheduler.add_job(update_news_daily, 'cron', hour=8, minute=0)
scheduler.start()

# Ch·∫°y trong n·ªÅn
try:
    print("üì∞ News update scheduler started...")
    while True:
        import time
        time.sleep(1)
except KeyboardInterrupt:
    scheduler.shutdown()
```

**Ch·∫°y:**
```bash
python daily_update.py
```

---

### V√≠ D·ª• 2: Build Historical Database (Sitemap)

```python
# build_database.py
from vnstock_news import AsyncBatchCrawler, SITES_CONFIG
from datetime import datetime, timedelta
import asyncio
import pandas as pd

async def build_database():
    """X√¢y d·ª±ng database tin t·ª©c t·ª´ 3 th√°ng g·∫ßn ƒë√¢y"""
    
    sites = ["cafef", "tuoitre", "vietstock"]
    all_articles = []
    
    for site_name in sites:
        print(f"‚è≥ Building {site_name}...")
        
        crawler = AsyncBatchCrawler(
            site_name=site_name,
            max_concurrency=5,
            request_delay=1.0
        )
        
        try:
            articles = await crawler.fetch_articles_async(
                site_name=site_name,
                max_articles=1000  # L·∫•y t·ªëi ƒëa 1000 b√†i
            )
            articles['source'] = site_name
            all_articles.append(articles)
            
            print(f"‚úÖ {site_name}: {len(articles)} articles")
            print(f"   Date range: {articles['publish_time'].min()} to {articles['publish_time'].max()}")
            
        except Exception as e:
            print(f"‚ùå {site_name}: {e}")
    
    if all_articles:
        result = pd.concat(all_articles, ignore_index=True)
        
        # X√≥a duplicate
        result = result.drop_duplicates(subset=['url'])
        
        # L∆∞u
        result.to_csv("news_database_3months.csv", index=False, encoding='utf-8-sig')
        
        print(f"\n‚úÖ Database built: {len(result)} articles")
        print(f"   Date range: {result['publish_time'].min()} to {result['publish_time'].max()}")
        print(f"   Sources: {result['source'].value_counts().to_dict()}")

# Ch·∫°y
asyncio.run(build_database())
```

---

## 2. Rate Limiting & Tr√°nh Block IP

### Chi·∫øn L∆∞·ª£c 1: Th√™m Delay

```python
from vnstock_news import BatchCrawler
import time

crawler = BatchCrawler(
    site_name="cafef",
    request_delay=2.0,  # Delay 2 gi√¢y gi·ªØa m·ªói request
    timeout=30
)

articles = crawler.fetch_articles(limit=500)
```

**H∆∞·ªõng d·∫´n:**
- `request_delay=0.5`: Nhanh nh·∫•t, r·ªßi ro block
- `request_delay=1.0`: C√¢n b·∫±ng
- `request_delay=2.0`: An to√†n, ch·∫≠p nh·∫≠n ch·∫≠m
- `request_delay=5.0`: R·∫•t an to√†n, r·∫•t ch·∫≠m

---

### Chi·∫øn L∆∞·ª£c 2: Gi·∫£m Concurrency

```python
import asyncio
from vnstock_news import AsyncBatchCrawler

async def safe_fetch():
    crawler = AsyncBatchCrawler(
        site_name="cafef",
        max_concurrency=2,    # Ch·ªâ 2 requests c√πng l√∫c
        request_delay=1.0
    )
    
    articles = await crawler.fetch_articles_async(
        site_name="cafef",
        max_articles=500
    )
    
    return articles

articles = asyncio.run(safe_fetch())
```

---

### Chi·∫øn L∆∞·ª£c 3: X·ª≠ L√Ω Rate Limit

```python
from vnstock_news import BatchCrawler
from requests.exceptions import HTTPError
import time

def fetch_with_retry(site_name, limit):
    """Fetch v·ªõi x·ª≠ l√Ω rate limit"""
    
    crawler = BatchCrawler(
        site_name=site_name,
        request_delay=1.5,
        timeout=30
    )
    
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            articles = crawler.fetch_articles(limit=limit)
            return articles
            
        except HTTPError as e:
            if e.response.status_code == 429:  # Rate limit
                retry_count += 1
                wait_time = 3600 * retry_count  # Ch·ªù 1h, 2h, 3h
                print(f"‚ö†Ô∏è Rate limited, waiting {wait_time}s before retry {retry_count}...")
                time.sleep(wait_time)
            else:
                raise
    
    raise Exception(f"Failed after {max_retries} retries")

# S·ª≠ d·ª•ng
articles = fetch_with_retry("cafef", limit=500)
```

---

### Chi·∫øn L∆∞·ª£c 4: Ki·ªÉm Tra Robots.txt

```python
import requests
from urllib.parse import urljoin

def check_robots_txt(domain):
    """Ki·ªÉm tra robots.txt"""
    
    try:
        robots_url = urljoin(domain, '/robots.txt')
        resp = requests.get(robots_url, timeout=10)
        
        rules = {
            'disallow': [],
            'crawl_delay': None
        }
        
        for line in resp.text.split('\n'):
            line = line.strip()
            
            if line.lower().startswith('disallow:'):
                path = line.split(':', 1)[1].strip()
                rules['disallow'].append(path)
            
            elif line.lower().startswith('crawl-delay:'):
                delay = float(line.split(':', 1)[1].strip())
                rules['crawl_delay'] = delay
        
        print(f"Robots.txt of {domain}:")
        print(f"  Disallow: {rules['disallow']}")
        print(f"  Crawl-delay: {rules['crawl_delay']}")
        
        return rules
        
    except Exception as e:
        print(f"Cannot fetch robots.txt: {e}")
        return None

# S·ª≠ d·ª•ng
rules = check_robots_txt("https://cafef.vn")
```

---

## 3. Caching & Performance

### B·∫≠t Caching

```python
from vnstock_news import EnhancedNewsCrawler
import asyncio

async def fetch_with_cache():
    crawler = EnhancedNewsCrawler(
        cache_enabled=True,
        cache_ttl=7200,           # Cache 2 gi·ªù
        cache_dir="./news_cache"  # Th∆∞ m·ª•c cache
    )
    
    # L·∫ßn ƒë·∫ßu: t·∫£i t·ª´ web, l∆∞u cache
    articles1 = await crawler.fetch_articles_async(
        sources=["https://cafef.vn/latest-news-sitemap.xml"],
        site_name="cafef",
        max_articles=100
    )
    
    # L·∫ßn th·ª© 2 (trong 2 gi·ªù): t·∫£i t·ª´ cache, nhanh h∆°n
    articles2 = await crawler.fetch_articles_async(
        sources=["https://cafef.vn/latest-news-sitemap.xml"],
        site_name="cafef",
        max_articles=100
    )
    
    return articles2

articles = asyncio.run(fetch_with_cache())
```

---

### X√≥a Cache C≈©

```python
import os
from pathlib import Path
from datetime import datetime, timedelta

def clean_old_cache(cache_dir="./news_cache", days=7):
    """X√≥a cache c≈© h∆°n N ng√†y"""
    
    cutoff_time = datetime.now() - timedelta(days=days)
    deleted_count = 0
    
    for file_path in Path(cache_dir).iterdir():
        if file_path.is_file():
            file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            
            if file_time < cutoff_time:
                file_path.unlink()
                deleted_count += 1
                print(f"üóëÔ∏è Deleted: {file_path.name}")
    
    print(f"‚úÖ Cleaned {deleted_count} cache files")

# Ch·∫°y h√†ng ng√†y
clean_old_cache(cache_dir="./news_cache", days=7)
```

---

## 4. Error Handling & Resumption

### X·ª≠ L√Ω L·ªói C∆° B·∫£n

```python
from vnstock_news import Crawler
from requests.exceptions import (
    RequestException, Timeout, ConnectionError
)
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def fetch_safely(site_name, method='feed'):
    """Fetch v·ªõi x·ª≠ l√Ω l·ªói"""
    
    try:
        crawler = Crawler(site_name=site_name)
        
        if method == 'feed':
            articles = crawler.get_articles_from_feed(limit_per_feed=20)
        elif method == 'sitemap':
            articles = crawler.get_articles_from_sitemap(limit=100)
        
        logger.info(f"‚úÖ {site_name}: {len(articles)} articles")
        return articles
        
    except ConnectionError:
        logger.error(f"‚ùå Connection error for {site_name}")
        return None
    
    except Timeout:
        logger.error(f"‚ùå Timeout for {site_name}")
        return None
    
    except Exception as e:
        logger.error(f"‚ùå Unexpected error for {site_name}: {e}")
        return None

# S·ª≠ d·ª•ng
for site in ["cafef", "tuoitre", "vietstock"]:
    articles = fetch_safely(site)
    if articles is not None:
        articles.to_csv(f"{site}_articles.csv", index=False)
```

---

### Resume T·ª´ N∆°i D·ª´ng

```python
from vnstock_news import BatchCrawler
import pandas as pd
import os

def fetch_with_resume(site_name, limit=1000):
    """Fetch t·ª´ sitemap, c√≥ kh·∫£ nƒÉng resume"""
    
    # Ki·ªÉm tra progress file
    progress_file = f"progress_{site_name}.txt"
    start_index = 0
    
    if os.path.exists(progress_file):
        with open(progress_file, 'r') as f:
            start_index = int(f.read().strip())
        print(f"üìå Resuming from index {start_index}")
    
    # T·∫°o crawler
    crawler = BatchCrawler(
        site_name=site_name,
        request_delay=1.5
    )
    
    all_articles = []
    
    # Load existing data n·∫øu c√≥
    output_file = f"{site_name}_articles.csv"
    if os.path.exists(output_file):
        all_articles = [pd.read_csv(output_file)]
    
    try:
        # Fetch t·ª´ start_index
        new_articles = crawler.fetch_articles(limit=limit)
        all_articles.append(new_articles)
        
        # G·ªôp v√† l∆∞u
        result = pd.concat(all_articles, ignore_index=True)
        result = result.drop_duplicates(subset=['url'])
        result.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        # Update progress
        with open(progress_file, 'w') as f:
            f.write(str(start_index + len(new_articles)))
        
        print(f"‚úÖ Fetched {len(new_articles)} new articles")
        print(f"üìä Total: {len(result)} articles")
        
    except KeyboardInterrupt:
        print("‚ö†Ô∏è Interrupted by user")
        # Progress ƒë√£ l∆∞u, c√≥ th·ªÉ resume l·∫°i
    
    except Exception as e:
        print(f"‚ùå Error: {e}")

# S·ª≠ d·ª•ng
fetch_with_resume("cafef", limit=2000)
```

---

## 5. Deduplication & Data Cleaning

### Lo·∫°i B·ªè B√†i Tr√πng

```python
import pandas as pd

def deduplicate_articles(df):
    """Lo·∫°i b·ªè b√†i tr√πng"""
    
    before = len(df)
    
    # C√°ch 1: Drop duplicate URLs
    df = df.drop_duplicates(subset=['url'])
    
    # C√°ch 2: Drop duplicate titles (n·∫øu ti√™u ƒë·ªÅ gi·ªëng h·ªát)
    df = df.drop_duplicates(subset=['title'])
    
    after = len(df)
    
    print(f"‚úÖ Removed {before - after} duplicates")
    print(f"üìä Remaining: {after} articles")
    
    return df

# S·ª≠ d·ª•ng
articles = pd.read_csv("all_articles.csv")
articles = deduplicate_articles(articles)
articles.to_csv("all_articles_dedup.csv", index=False)
```

---

### L√†m S·∫°ch N·ªôi Dung

```python
import re
import pandas as pd

def clean_content(text):
    """L√†m s·∫°ch n·ªôi dung"""
    
    if pd.isna(text):
        return ""
    
    # X√≥a HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # X√≥a c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt
    text = re.sub(r'[\n\r\t]+', ' ', text)
    
    # X√≥a kho·∫£ng tr·∫Øng th·ª´a
    text = re.sub(r'\s+', ' ', text).strip()
    
    # X√≥a c√°c emoji
    text = re.sub(r'[^\w\s\u0100-\uFFFF.,!?;:\'-]', '', text)
    
    return text

def clean_articles(df):
    """L√†m s·∫°ch to√†n b·ªô DataFrame"""
    
    for col in ['title', 'short_description', 'content']:
        if col in df.columns:
            df[col] = df[col].apply(clean_content)
    
    return df

# S·ª≠ d·ª•ng
articles = pd.read_csv("raw_articles.csv")
articles = clean_articles(articles)
articles.to_csv("clean_articles.csv", index=False)
```

---

## 6. Timezone Handling

### Chu·∫©n H√≥a Timezone

```python
import pandas as pd
from datetime import datetime
import pytz

def normalize_timezone(df, target_tz='Asia/Ho_Chi_Minh'):
    """Chu·∫©n h√≥a timezone"""
    
    # Convert sang datetime
    df['publish_time'] = pd.to_datetime(df['publish_time'])
    
    # ƒê·∫∑t timezone n·∫øu ch∆∞a c√≥
    if df['publish_time'].dt.tz is None:
        df['publish_time'] = df['publish_time'].dt.tz_localize('UTC')
    
    # Convert sang target timezone
    df['publish_time'] = df['publish_time'].dt.tz_convert(target_tz)
    
    return df

# S·ª≠ d·ª•ng
articles = pd.read_csv("articles.csv")
articles = normalize_timezone(articles, target_tz='Asia/Ho_Chi_Minh')
print(articles['publish_time'].head())
```

**Output:**
```
0   2025-01-15 10:30:00+07:00
1   2025-01-15 09:15:00+07:00
```

---

## 7. Production Deployment

### V√≠ D·ª•: News Aggregator Service

```python
# news_service.py
from vnstock_news import AsyncBatchCrawler, TrendingAnalyzer
from vnstock_news.api.enhanced import EnhancedNewsCrawler
import asyncio
import pandas as pd
from datetime import datetime
import logging
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class NewsAggregatorService:
    """Service ƒë·ªÉ t·∫≠p h·ª£p tin t·ª©c t·ª´ nhi·ªÅu b√°o"""
    
    def __init__(self, output_dir="./news_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.sites = ["cafef", "tuoitre", "vietstock"]
        self.analyzer = TrendingAnalyzer()
    
    async def fetch_all_news(self):
        """L·∫•y tin t·ª´ t·∫•t c·∫£ b√°o"""
        
        logger.info("Starting news fetch...")
        all_articles = []
        
        for site_name in self.sites:
            try:
                logger.info(f"Fetching from {site_name}...")
                
                crawler = AsyncBatchCrawler(
                    site_name=site_name,
                    max_concurrency=3,
                    request_delay=1.0
                )
                
                articles = await crawler.fetch_articles_async(
                    site_name=site_name,
                    max_articles=100
                )
                
                articles['source'] = site_name
                all_articles.append(articles)
                
                logger.info(f"‚úÖ {site_name}: {len(articles)} articles")
                
            except Exception as e:
                logger.error(f"‚ùå {site_name}: {e}")
        
        if not all_articles:
            logger.error("No articles fetched")
            return None
        
        # G·ªôp
        result = pd.concat(all_articles, ignore_index=True)
        result = result.drop_duplicates(subset=['url'])
        
        logger.info(f"üìä Total: {len(result)} articles from {len(self.sites)} sources")
        
        return result
    
    async def analyze_trending(self, articles):
        """Ph√¢n t√≠ch trending"""
        
        logger.info("Analyzing trending keywords...")
        
        if articles is None or len(articles) == 0:
            logger.warning("No articles to analyze")
            return None
        
        keywords = self.analyzer.extract_keywords(
            articles['title'].tolist(),
            top_n=20
        )
        
        return keywords
    
    async def save_results(self, articles, keywords):
        """L∆∞u k·∫øt qu·∫£"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # L∆∞u articles
        articles_file = self.output_dir / f"articles_{timestamp}.csv"
        articles.to_csv(articles_file, index=False, encoding='utf-8-sig')
        logger.info(f"üìÅ Saved articles to {articles_file}")
        
        # L∆∞u keywords
        keywords_df = pd.DataFrame([
            {'keyword': k, 'frequency': v}
            for k, v in keywords.items()
        ])
        keywords_file = self.output_dir / f"keywords_{timestamp}.csv"
        keywords_df.to_csv(keywords_file, index=False)
        logger.info(f"üìÅ Saved keywords to {keywords_file}")
    
    async def run(self):
        """Ch·∫°y service"""
        
        try:
            articles = await self.fetch_all_news()
            
            if articles is not None:
                keywords = await self.analyze_trending(articles)
                
                if keywords is not None:
                    await self.save_results(articles, keywords)
                    
                    logger.info("‚úÖ Service completed successfully")
                    return True
        
        except Exception as e:
            logger.error(f"‚ùå Service failed: {e}")
            return False

# Ch·∫°y
if __name__ == "__main__":
    service = NewsAggregatorService()
    success = asyncio.run(service.run())
    
    exit(0 if success else 1)
```

**Ch·∫°y:**
```bash
python news_service.py
```

---

### Ch·∫°y ƒê·ªãnh K·ª≥ (Cron Job)

```bash
# Crontab: Ch·∫°y m·ªói ng√†y l√∫c 8:00 AM
0 8 * * * cd /path/to/project && python news_service.py >> logs/news_service.log 2>&1
```

---

## 8. Monitoring & Logging

### Setup Logging

```python
import logging
from logging.handlers import RotatingFileHandler
import os

def setup_logging(log_dir="./logs"):
    """Setup logging"""
    
    os.makedirs(log_dir, exist_ok=True)
    
    logger = logging.getLogger("vnstock_news")
    logger.setLevel(logging.DEBUG)
    
    # File handler
    fh = RotatingFileHandler(
        os.path.join(log_dir, "vnstock_news.log"),
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )
    fh.setLevel(logging.DEBUG)
    
    # Console handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    
    # Formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    
    logger.addHandler(fh)
    logger.addHandler(ch)
    
    return logger

# S·ª≠ d·ª•ng
logger = setup_logging()
logger.info("Starting news crawler...")
logger.error("Error occurred!")
```

---

## 9. Checklist - Tr∆∞·ªõc Khi Deploy

- [ ] Test v·ªõi d·ªØ li·ªáu th·ª±c
- [ ] Ki·ªÉm tra error handling
- [ ] C·∫•u h√¨nh request_delay ph√π h·ª£p
- [ ] Ki·ªÉm tra robots.txt
- [ ] B·∫≠t caching
- [ ] Setup logging
- [ ] Chu·∫©n b·ªã deduplication
- [ ] Test resume/retry logic
- [ ] Ki·ªÉm tra timezone
- [ ] C√≥ backup strategy
- [ ] C√≥ monitoring
- [ ] C√≥ alert n·∫øu l·ªói

---

## T·ªïng K·∫øt

**Best Practices:**
1. ‚úÖ Ch·ªçn ph∆∞∆°ng th·ª©c ph√π h·ª£p (RSS/Sitemap/Batch)
2. ‚úÖ Tu√¢n th·ªß rate limiting (request_delay, concurrency)
3. ‚úÖ B·∫≠t caching ƒë·ªÉ ti·∫øt ki·ªám bandwidth
4. ‚úÖ X·ª≠ l√Ω l·ªói & c√≥ kh·∫£ nƒÉng resume
5. ‚úÖ Lo·∫°i b·ªè duplicate & l√†m s·∫°ch d·ªØ li·ªáu
6. ‚úÖ Setup logging & monitoring
7. ‚úÖ Ki·ªÉm tra robots.txt & ToS
8. ‚úÖ Deploy v·ªõi scheduler (cron, APScheduler)
9. ‚úÖ Test k·ªπ tr∆∞·ªõc production
